## Relevant Knowledge Base Entries

### [THEOREM] Equality of determinant expansions (linalg_ii_ht2026.ch2.thm.1)  [relevance: 0.774]
**Statement:** Let A = (a_{ij}) be an n by n matrix and let C_{ij} denote the (i,j)-th cofactor of A. Then the determinant of A may be calculated by expanding along any column or row of A. So, for any i with 1 ≤ i ≤ n, we have det A equals a_{1i}C_{1i} + a_{2i}C_{2i} + ... + a_{ni}C_{ni} (this is expansion along the i-th column), and det A also equals a_{i1}C_{i1} + a_{i2}C_{i2} + ... + a_{in}C_{in} (this is expansion along the i-th row).
**Hypotheses:**
  - A is an n by n matrix with entries a_{ij}
  - C_{ij} is the (i,j)-th cofactor of A, defined as (-1)^{i+j} times the determinant of the (n-1) by (n-1) matrix obtained by deleting row i and column j from A
  - i is any integer with 1 ≤ i ≤ n
**Conclusion:** The determinant can be computed by cofactor expansion along any row or any column, and all 2n possible expansions give the same value
**When to use:** asked to compute a determinant and you see a row or column with many zeros — expand along that row/column; need to prove a property of determinants by induction on size — cofactor expansion gives the inductive step; working with block matrices and need to relate det of the whole to det of blocks — cofactor methods can help; see a proof that uses 'expand along the first column' and wonder if other columns work — yes, they all work
**Proof strategy:** Uniqueness argument using the three characterizing properties of determinants

### [THEOREM] Fundamental Properties of Determinants (linalg_ii_ht2026.ch2.thm.1)  [relevance: 0.736]
**Statement:** The determinant function defined in Definition 3 has the following properties. (A) det is linear in each row: if matrix C is obtained from matrix A by replacing row i with λ r_i + μ v (where v is another vector), and matrix B is A with row i replaced by v, then det(C) = λ det(A) + μ det(B), holding all other rows fixed. (B) If matrix A has two equal rows (r_i = r_j for i ≠ j), then det A = 0. (B') If matrix B is obtained from A by swapping two different rows, then det B = -det A.
**Hypotheses:**
  - det is defined as in Definition 3 (the recursive/inductive definition)
**Conclusion:** det satisfies multilinearity (property A), vanishes on matrices with repeated rows (property B), and alternates sign under row swaps (property B')
**When to use:** Proving almost any property of determinants — these are the foundational axioms; Showing det is well-defined under different characterizations; Justifying why row operations affect det in predictable ways; Connecting det to linear dependence (repeated rows → zero det → dependent rows)
**Proof strategy:** Induction on n for properties (A) and (B), with a lemma showing (A)+(B) ⟺ (A)+(B')

### [THEOREM] Fundamental Properties of Determinants (linalg_ii_ht2026.ch2.thm.1)  [relevance: 0.736]
**Statement:** The determinant function defined in Definition 3 has the following properties. (A) det is linear in each row: if matrix C is obtained from matrix A by replacing row i with λ r_i + μ v (where v is another vector), and matrix B is A with row i replaced by v, then det(C) = λ det(A) + μ det(B), holding all other rows fixed. (B) If matrix A has two equal rows (r_i = r_j for i ≠ j), then det A = 0. (B') If matrix B is obtained from A by swapping two different rows, then det B = -det A.
**Hypotheses:**
  - det is defined as in Definition 3 (the recursive/inductive definition)
**Conclusion:** det satisfies multilinearity (property A), vanishes on matrices with repeated rows (property B), and alternates sign under row swaps (property B')
**When to use:** Proving almost any property of determinants — these are the foundational axioms; Showing det is well-defined under different characterizations; Justifying why row operations affect det in predictable ways; Connecting det to linear dependence (repeated rows → zero det → dependent rows)
**Proof strategy:** Induction on n for properties (A) and (B), with a lemma showing (A)+(B) ⟺ (A)+(B')

### [THEOREM] Gram-Schmidt Orthogonalization Process (linalg_ii_ht2026.ch4.thm.2)  [relevance: 0.658]
**Statement:** Let v₁, ..., vₖ be independent vectors in R^n (column vectors) or R^n. Then there are orthonormal vectors w₁, ..., wₖ such that, for each 1 ≤ i ≤ k, we have the span of w₁, ..., wᵢ equals the span of v₁, ..., vᵢ.
**Hypotheses:**
  - v₁, ..., vₖ are vectors in R^n
  - The vectors v₁, ..., vₖ are linearly independent
**Conclusion:** There exist orthonormal vectors w₁, ..., wₖ that span the same nested subspaces as v₁, ..., vᵢ for each i
**When to use:** Need to convert a basis to an orthonormal basis; Want to orthogonally diagonalize a symmetric matrix with repeated eigenvalues; Need an orthonormal basis for a subspace; Working with inner products and need perpendicular vectors
**Proof strategy:** Induction on i: at each step, project vᵢ₊₁ onto the orthogonal complement of span(w₁,...,wᵢ) and normalize

### [THEOREM] Gram-Schmidt Orthogonalization Process (linalg_ii_ht2026.ch4.thm.2)  [relevance: 0.658]
**Statement:** Let v₁, ..., vₖ be independent vectors in R^n (column vectors) or R^n. Then there are orthonormal vectors w₁, ..., wₖ such that, for each 1 ≤ i ≤ k, we have the span of w₁, ..., wᵢ equals the span of v₁, ..., vᵢ.
**Hypotheses:**
  - v₁, ..., vₖ are vectors in R^n
  - The vectors v₁, ..., vₖ are linearly independent
**Conclusion:** There exist orthonormal vectors w₁, ..., wₖ that span the same nested subspaces as v₁, ..., vᵢ for each i
**When to use:** Need to convert a basis to an orthonormal basis; Want to orthogonally diagonalize a symmetric matrix with repeated eigenvalues; Need an orthonormal basis for a subspace; Working with inner products and need perpendicular vectors
**Proof strategy:** Induction on i: at each step, project vᵢ₊₁ onto the orthogonal complement of span(w₁,...,wᵢ) and normalize

### [THEOREM] Singular Value Decomposition (linalg_ii_ht2026.ch4.thm.1)  [relevance: 0.658]
**Statement:** Let A be an m by n matrix of rank r. Then there exist an orthogonal m by m matrix P and an orthogonal n by n matrix Q such that PAQ equals a block matrix with D in the top-left corner and zeros elsewhere, where D is an invertible diagonal r by r matrix with positive entries listed in decreasing order.
**Hypotheses:**
  - A is an m by n matrix
  - A has rank r
**Conclusion:** There exist orthogonal matrices P (m by m) and Q (n by n) such that PAQ is in block diagonal form with a positive diagonal matrix D of size r by r in the top-left block and zeros elsewhere
**When to use:** Need to analyze a non-square matrix or rank-deficient matrix; Problem involves least squares, data compression, or low-rank approximation; Need to compute the pseudoinverse or find closest rank-k approximation; Want to understand the geometric action of a linear transformation; Numerical computation requiring stable matrix factorization; Principal component analysis or dimensionality reduction
**Proof strategy:** Apply spectral theorem to the symmetric matrix A^T A, construct P1 from the eigenvector decomposition, then extend to full orthogonal matrix P

### [DEFINITION] Quadratic Form (linalg_ii_ht2026.ch4.def.3)  [relevance: 0.578]
**Statement:** A quadratic form in n variables x₁, x₂, ..., xₙ is a polynomial where each term has degree two. That is, it can be written as a sum of aᵢⱼ xᵢ xⱼ over i ≤ j, where the aᵢⱼ are scalars. Thus a quadratic form in two variables x, y is a x squared plus b x y plus c y squared where a, b, c are scalars. The following is a coordinate-free way of defining quadratic forms. A quadratic form on a vector space V equals B(v,v) where B: V × V → R is a bilinear map. The connection with symmetric matrices is that we can write the sum as x transpose A x, where x is the column vector of variables and A is a symmetric matrix with diagonal entries aᵢᵢ and off-diagonal entries (1/2)aᵢⱼ.
**Hypotheses:**
  - Variables x₁, ..., xₙ or vector space V
**When to use:** Working with degree 2 polynomials in multiple variables; Analyzing conics or quadrics; Studying second-order behavior (Hessians, curvature); Optimization with quadratic objectives

### [DEFINITION] Quadratic Form (linalg_ii_ht2026.ch4.def.3)  [relevance: 0.578]
**Statement:** A quadratic form in n variables x₁, x₂, ..., xₙ is a polynomial where each term has degree two. That is, it can be written as a sum of aᵢⱼ xᵢ xⱼ over i ≤ j, where the aᵢⱼ are scalars. Thus a quadratic form in two variables x, y is a x squared plus b x y plus c y squared where a, b, c are scalars. The following is a coordinate-free way of defining quadratic forms. A quadratic form on a vector space V equals B(v,v) where B: V × V → R is a bilinear map. The connection with symmetric matrices is that we can write the sum as x transpose A x, where x is the column vector of variables and A is a symmetric matrix with diagonal entries aᵢᵢ and off-diagonal entries (1/2)aᵢⱼ.
**Hypotheses:**
  - Variables x₁, ..., xₙ or vector space V
**When to use:** Working with degree 2 polynomials in multiple variables; Analyzing conics or quadrics; Studying second-order behavior (Hessians, curvature); Optimization with quadratic objectives

### [PROPOSITION] Determinant uniquely determined by algebraic properties (linalg_ii_ht2026.ch2.prop.1)  [relevance: 0.564]
**Statement:** The function det is entirely determined by the three algebraic properties: (i) det is linear in the rows of a matrix, (ii) if a matrix has two equal rows then its determinant is zero, (iii) det I_n = 1. Further, the determinant of an n by n matrix A = (a_{ij}) equals the sum over all permutation matrices P_{i_1...i_n} of (det P_{i_1...i_n}) times (a_{1i_1} times ... times a_{ni_n}), where P_{i_1...i_n} has rows e_{i_1}, ..., e_{i_n}.
**Hypotheses:**
  - A is an n by n matrix with entries a_{ij}
  - det satisfies (i) multilinearity in rows, (ii) alternating property, (iii) normalization det I_n = 1
**Conclusion:** The determinant function is uniquely determined and equals the sum over all permutation matrices of signed monomials
**When to use:** need to prove a property holds for all determinants by checking it satisfies the three axioms; want to derive an explicit formula for det A from first principles; asked to show that det is the unique function with certain properties; working with abstract multilinear algebra and need a concrete handle on det
**Proof strategy:** Constructive expansion using multilinearity

### [THEOREM] Spectral Theorem (informal statement) (linalg_ii_ht2026.ch4.thm.1)  [relevance: 0.542]
**Statement:** Let A be an n by n symmetric matrix. Then the roots of the characteristic polynomial of A are real and A has an eigenbasis consisting of mutually perpendicular unit vectors. That is, A has an orthonormal eigenbasis.
**Hypotheses:**
  - A is an n by n real matrix
  - A is symmetric, meaning A = A^T
**Conclusion:** The eigenvalues of A are real, and A has an orthonormal eigenbasis
**When to use:** Working with a symmetric matrix and need to diagonalize it; Want to understand geometric meaning of a quadratic form; Need to find principal axes of an ellipse, ellipsoid, or other conic/quadric; Analyzing a system where the matrix respects the inner product structure

### [THEOREM] Spectral Theorem (informal statement) (linalg_ii_ht2026.ch4.thm.1)  [relevance: 0.542]
**Statement:** Let A be an n by n symmetric matrix. Then the roots of the characteristic polynomial of A are real and A has an eigenbasis consisting of mutually perpendicular unit vectors. That is, A has an orthonormal eigenbasis.
**Hypotheses:**
  - A is an n by n real matrix
  - A is symmetric, meaning A = A^T
**Conclusion:** The eigenvalues of A are real, and A has an orthonormal eigenbasis
**When to use:** Working with a symmetric matrix and need to diagonalize it; Want to understand geometric meaning of a quadratic form; Need to find principal axes of an ellipse, ellipsoid, or other conic/quadric; Analyzing a system where the matrix respects the inner product structure

### [EXAMPLE] Inertia Matrix in Dynamics (linalg_ii_ht2026.ch4.eg.5)  [relevance: 0.531]
**Statement:** A rigid body, rotating about a fixed point O with angular velocity omega has kinetic energy T equals one half omega transpose I_0 omega, where I_0 is the inertia matrix (a 3 by 3 symmetric matrix with diagonal entries A, B, C and off-diagonal entries involving products of inertia D, E, F). The diagonal entries are integrals of density times sums of squared coordinates, and off-diagonal entries are integrals of density times products of coordinates. For a spinning top, symmetrical about its axis with O on the axis, the eigenvectors of I_0 are along the axis with two eigenvectors orthogonal to that. With respect to this basis I_0 equals diag(A, A, C), but the spectral theorem applies to any rigid body, however irregular the distribution of matter.
**Hypotheses:**
  - Rigid body rotating about fixed point O
**When to use:** Studying rigid body dynamics; Computing principal axes of inertia; Understanding rotational kinetic energy; Working with moment of inertia tensors

### [EXAMPLE] Inertia Matrix in Dynamics (linalg_ii_ht2026.ch4.eg.5)  [relevance: 0.531]
**Statement:** A rigid body, rotating about a fixed point O with angular velocity omega has kinetic energy T equals one half omega transpose I_0 omega, where I_0 is the inertia matrix (a 3 by 3 symmetric matrix with diagonal entries A, B, C and off-diagonal entries involving products of inertia D, E, F). The diagonal entries are integrals of density times sums of squared coordinates, and off-diagonal entries are integrals of density times products of coordinates. For a spinning top, symmetrical about its axis with O on the axis, the eigenvectors of I_0 are along the axis with two eigenvectors orthogonal to that. With respect to this basis I_0 equals diag(A, A, C), but the spectral theorem applies to any rigid body, however irregular the distribution of matter.
**Hypotheses:**
  - Rigid body rotating about fixed point O
**When to use:** Studying rigid body dynamics; Computing principal axes of inertia; Understanding rotational kinetic energy; Working with moment of inertia tensors

### [REMARK] Diagonalizability over General Fields (linalg_ii_ht2026.ch3.rmk.3)  [relevance: 0.523]
**Statement:** We can decide on the diagonalizability of a matrix over a general field by following the same procedures. First all the roots of the characteristic polynomial need to be in the field, and then for each eigenvalue the algebraic multiplicity needs to equal the geometric multiplicity. The matrix B equals ((1,-1),(1,1)) has characteristic polynomial (x-1) squared plus 1 equals x squared minus 2x plus 2. Over the complex numbers this is diagonalizable as B has distinct roots 1 plus or minus i. The same is true over the field Q of i equals {q_1 plus q_2 times i where q_1, q_2 in rationals}. Over the reals and rationals the characteristic polynomial has no roots so B is not diagonalizable. Over Z_2 the characteristic polynomial equals x squared but the 0-eigenspace has dimension 1, so geometric multiplicity 1 is less than algebraic multiplicity 2, making B not diagonalizable. Over Z_3 the polynomial has no roots so B is not diagonalizable. Over Z_5 we have -1 equals 4 equals 2 squared so the polynomial factors as (x+1)(x-3) and B has distinct eigenvalues hence is diagonalizable.
**Hypotheses:**
  - Working over an arbitrary field
**When to use:** Working with matrices over finite fields; Need to understand field-dependence of eigenvalue theory; Studying linear algebra over fields other than reals or complex numbers

### [REMARK] Diagonalizability over General Fields (linalg_ii_ht2026.ch3.rmk.3)  [relevance: 0.523]
**Statement:** We can decide on the diagonalizability of a matrix over a general field by following the same procedures. First all the roots of the characteristic polynomial need to be in the field, and then for each eigenvalue the algebraic multiplicity needs to equal the geometric multiplicity. The matrix B equals ((1,-1),(1,1)) has characteristic polynomial (x-1) squared plus 1 equals x squared minus 2x plus 2. Over the complex numbers this is diagonalizable as B has distinct roots 1 plus or minus i. The same is true over the field Q of i equals {q_1 plus q_2 times i where q_1, q_2 in rationals}. Over the reals and rationals the characteristic polynomial has no roots so B is not diagonalizable. Over Z_2 the characteristic polynomial equals x squared but the 0-eigenspace has dimension 1, so geometric multiplicity 1 is less than algebraic multiplicity 2, making B not diagonalizable. Over Z_3 the polynomial has no roots so B is not diagonalizable. Over Z_5 we have -1 equals 4 equals 2 squared so the polynomial factors as (x+1)(x-3) and B has distinct eigenvalues hence is diagonalizable.
**Hypotheses:**
  - Working over an arbitrary field
**When to use:** Working with matrices over finite fields; Need to understand field-dependence of eigenvalue theory; Studying linear algebra over fields other than reals or complex numbers
